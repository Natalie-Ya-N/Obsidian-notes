---
Date: 2024-01-04
URL: https://arxiv.org/pdf/2401.01313.pdf
Share: "True"
---
---



这是当前页面第一部分的摘要：

- **文章主题**：本文对大型语言模型（LLMs）中的幻觉缓解技术进行了全面的调查，幻觉是指模型生成的内容看似真实，但与事实不符的现象。
- **文章动机**：幻觉是影响LLMs在实际应用中安全部署的最大障碍之一，需要寻找有效的方法来检测和减少幻觉，提高模型的可靠性和准确性。
- **文章贡献**：本文提出了一个系统的分类体系，将幻觉缓解技术分为不同的类别，如提示工程、模型开发等，并分析了各种技术的特点、优势和局限性。
- **文章结论**：本文不仅揭示了LLMs中幻觉的严重性，也整合和组织了多种缓解技术，为计算语言学领域的知识进步做出了贡献。

这是当前网页的第二部分的摘要：

- 本部分讨论了减轻大型语言模型（LLM）中幻觉的各种方法，包括后生成细化、提示调整、新模型开发和监督微调等。
- 本部分指出，幻觉缓解在LLM中是一个多方面的挑战，需要一系列创新的技术来应对幻觉的细微差别。
- 本部分展望了未来的发展和改进的方向，如混合模型的构建、无监督或弱监督学习技术的探索、道德后果和社会影响的考量等。
- 本部分最后通过表1对所有涉及幻觉缓解的工作进行了总结和分类。

---


摘要(Abstract):  
随着大型语言模型(LLMs)在生成类似人类的文本方面的能力不断进步,一个关键挑战仍然是它们  
倾向于“幻觉”-一生成看似事实但无根据的内容。这种幻觉问题可以说是将这些强大的LLMs安全部  
署到影响人们生活的现实世界生产系统中的最大障碍。LLMs在实际应用中的广泛采用之路在很大程  
度上依赖于解决和缓解幻觉问题。与专注于有限任务的传统AI系统不同,LLMs在训练过程中接触了  
大量的在线文本数据。虽然这使它们能够展示出令人印象深刻的语言流畅性,但也意味着它们能够从  
训练数据的偏见中推断信息,误解模糊的提示,或修改信息以表面上与输入对齐。当我们依赖语言生  
成能力用于敏感应用,如总结医疗记录、客户支持对话、财务分析报告以及提供错误的法律建议时,  
这变得极为令人担忧。小错误可能导致伤害,揭示了LLMs尽管在自我学习方面有所进步,但缺乏实  
际理解。本文对超过三十二种缓解LLMs中幻觉的技术进行了全面调查。其中值得注意的包括检索增  
强生成(RAG)(Lewis等人,2021)、知识检索(Varshney等人,2023)、CoNLI(Lei等人,  
2023)和CoVe(Dhuliawala等人,2023)。此外,我们引入了一个详细的分类法,根据各种参  
数,如数据集利用、常见任务、反馈机制和检索器类型,对这些方法进行了分类。这种分类有助于区  
分专门设计用于解决LLMs中幻觉问题的不同方法。此外,我们还分析了这些技术内在的挑战和局限  
性,并提出了潜在的解决方案和未来研究的建议方向。  

结论(Conclusion):  
LLMs中的幻觉缓解代表了一种多方面的挑战,通过一系列创新技术来解决。讨论的方法,从后生成  
精炼到监督微调,强调了解决LLMs幻觉问题的严重性,并迫切需要全面解决方案。在后生成精炼领  
域,RARR脱颖而出,自动化了归因过程并使内容与检索证据对齐。高熵词检测和替换解决了LLM生  
成内容中由高熵词引起的幻觉,展示了上下文感知替换的重要性。通过反馈和推理进行自我完善带来  
了像ChatProtect这样的有影响力的策略,专注于自我矛盾检测,以及在医疗生成QA系统中采用迭  
代反馈过程的自我反思方法。结构化比较推理引入了一种结构化的方法来进行文本偏好预测,增强了  
一致性并减少了幻觉。提示调整作为强大的技术出现,UPRISE展示了基于提示的调整的通用性。  
SynTra通过合成任务来缓解抽象总结中的幻觉,提供了可扩展性,但引发了关于与人类反馈相比的  
有效性的问题。开发新模型强调了解码策略,如CAD和DoLa,两者在通过指导生成阶段来减少幻觉  
方面都起到了重要作用。KG利用和基于忠实度的损失函数也在方法如RHO和THAM框架中发挥了关  
键作用。监督微调是调整LLMs以适应下游任务的关键阶段,通过各种镜头探索,如知识注入和教  
师-学生方法,其中将领域特定知识注入较弱的LLMs,以及像HAR这样的方法使用反事实数据集来  
提高事实性。通过这些方法,本文不仅揭示了LLMs中幻觉的严重性,而且整合和组织了多样化的缓  
解技术,为缓解幻觉领域未来的研究做出了贡献。此外,本文讨论了这些技术的固有局限性和挑战,  
并提出了未来研究的方向。本质上,本文不仅揭示了LLMs中幻觉的严重性,而且整合和组织了多样  
化的缓解技术,为缓解幻觉领域未来的研究做出了贡献。此外,本文讨论了这些技术的固有局限性和  
挑战,并提出了未来研究的方向。本质上,本文不仅揭示了LLMs中幻觉的严重性,而且整合和组织  
了多样化的缓解技术,为缓解幻觉领域未来的研究做出了贡献。此外,本文讨论了这些技术的固有局  
限性和挑战,并提出了未来研究的方向。  

1.主要解决了什么问题?

·文档主要解决了大型语言模型(LLMs)在生成文本时出现的“幻觉”问题,即模型生成看似事实  
但无根据的内容。这个问题在将LLMs应用于敏感领域(如医疗记录总结、客户支持对话、财务  
分析报告等)时尤为突出,因为小错误可能导致严重后果。  

2.提出了什么解决方案?

。文档提出了超过三十二种缓解LLMs中幻觉的技术,包括检索增强生成(RAG)、知识检  
索、CoNLI、CoVe等方法。这些技术旨在通过不同的策略来减少模型生成错误信息的可能  
性。  

3.解决方案中核心的方法/步骤/策略是什么?

。 文档中的核心方法包括:

■ 提前生成(Before Generation):在生成文本之前进行信息检索,以验证和减少幻  
觉。  
■ 生成过程中(During Generation):在生成每个句子时进行信息检索,以确保内容的  
准确性。  
■ 生成后(After Generation):在生成完整文本后进行信息检索,以纠正可能的幻觉。  
■ 自我完善通过反馈和推理(Self Refinement through Feedback and Reasoning):  
通过反馈和推理来提高模型的可靠性。  
■提示调整(Prompt Tuning):调整模型在微调阶段的指令,以提高其在特定任务上的  
效果。  
■ 引入新的解码策略(Introducing New Decoding Strategy):设计特定的解码技术来  
减少幻觉。  
■ 利用知识图谱(Utilization of Knowledge Graph):使用知识图谱来增强模型的推理  
能力。  
■ 引入基于忠实度的损失函数(Introducing Faithfulness-based Loss Function):创  
建衡量模型输出与输入数据或真实情况匹配程度的度量。  
■ 监督微调(Supervised Fine-tuning):使用标记数据对模型进行微调,以提高其在特  
定任务上的性能。  

4.结论是什么?

。 文档的结论强调了LLMs中幻觉问题的严重性,并指出了解决这一问题的迫切需求。通过整  
合和组织多样化的缓解技术,文档为缓解幻觉领域的未来研究奠定了基础,并提出了未来研  
究方向,包括开发混合模型、探索无监督或弱监督学习技术、以及考虑道德和社会影响。  

5.有什么限制条件?
文档中提到的一些限制条件包括:
- 某些方法依赖于外部知识源,可能需要额外的资源和计算成本。  
- 一些技术可能在特定任务或领域上表现更好,而不一定适用于所有情况。  
- 某些方法可能需要大量的标记数据进行微调,这可能不总是可行或经济的。  
- 一些方法可能在处理长文档或复杂任务时遇到困难。  
- 一些方法可能需要额外的人工参与,如在反馈和推理过程中。  
- 某些方法可能无法完全解决幻觉问题,因为幻觉的根源可能与模型的训练数据或生成机制有关。  



三十二种缓解LLMs中幻觉的技术：

  

提示工程领域：

A. 提前检索增强生成（RAG）

- 生成之前：在文本生成之前进行信息检索的策略，比如LLM-Augmenter
- 生成过程中：在句子生成时进行检索，比如知识检索，D&Q框架
- 生成之后：在全文生成完成后进行检索，比如RARR
- 端到端：将检索和生成整合在一起的模型，比如原始的RAG模型

B. 通过反馈和推理进行自我完善

- 通过用户反馈迭代改进输出结果，比如Prompting GPT-3 for Reliability
- 发现并缓解自我矛盾，比如ChatProtect
- 通过反馈循环进行交互式改进，比如自我反思方法论

C. 提示调整

- 为模型提供调整指令，比如UPRISE, SynTra

模型开发领域：

A. 新解码策略

- 在生成过程中引导，比如上下文感知解码

B. 利用知识图谱

- 注入结构化知识，比如RHO

C. 基于忠实度的损失函数

- 提升输出内容的真实性，比如THAM框架

D. 监督微调

- 在标记数据上进行模型调整，比如知识注入方法

  

---

一个LLM提示词的内容框架。 组成部分：

指令：引导LLM推理格式和结构的简短提示  
理由：在上下文导向推理（CoT）过程中生成的中间推理步骤  
示例：展示目标推理模式的输入输出实例  
环境：交互式上下文，如操作系统、应用程序、网页代理  
工具：扩展LLM能力的外部模块，包括执行、知识或验证  

模块：

感知：通过CoT提示顺序解释环境状态  
记忆：短期存储暂时信息；长期保留静态知识  
推理：在交错的CoT格式中进行规划、决策和行动  

格式：

文本：标准CoT的顺序语言  
树形结构：表示互连思想的层级结构  
图形：映射思想之间关系的网络  
程序：基于代码的思想，逻辑与语言分离  
表格：以行/列表格方式展现连贯思维进程  

过程：

提示：用指令和示例引出目标推理格式  
聚合：结合多个CoT路径以提高流畅性  
验证：利用外部信息源评估和修订思想  
定制化：与特定用户需求相匹配  

实体：

问题：触发代理CoT推理的输入  
答案：从CoT推理中得出的最终输出  
行动：基于代理决策的操作执行  
情节：朝向目标的完整交互序列  
轮次：情节内单个顺序互动  

属性：

可解释性：理解导致结论的推理过程  
可控性：通过调整提示影响模型行为  
适应性：在新环境和任务中有效性  
安全性：确保行为安全，避免有害故障模式  

任务：

算术：数学推理  
文本：语言理解和常识  
视觉：结合图像的多模态推理  
符号：结构化输入，如编程语言  
通用：广泛的日常现实世界应用